{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning  Sinuists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Import the required libraries\n",
    "import numpy as np\n",
    "from keras import backend as K\n",
    "np.random.seed(1338)\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Preprocess and Normalise Dat\n",
    "img_rows, img_cols = 200, 200\n",
    "\n",
    "input_path_t=\"images/resized_data/training\"\n",
    "input_path_v=\"images/resized_data/validation\"\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    shape_ord = (1, img_rows, img_cols)\n",
    "else:  # channel_last\n",
    "    shape_ord = (img_rows, img_cols, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Dataset for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator,img_to_array\n",
    "\n",
    "listing = os.listdir(input_path_t)\n",
    "num_samples=len(listing)\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "        rotation_range=40,\n",
    "        width_shift_range=0.05,\n",
    "        height_shift_range=0.05,\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest')\n",
    "img_rows, img_cols = img_rows, img_cols\n",
    "\n",
    "\n",
    "    \n",
    "x = []\n",
    "images=[]\n",
    "files=[]\n",
    "c=0\n",
    "for file in listing:\n",
    "    img = cv2.imread(input_path_t+\"/\"+file,0)  # this is a PIL image\n",
    "    print(input_path_t+\"/\"+file)\n",
    "    #print(i+ mean)\n",
    "    images.append(img)\n",
    "    files.append(file)\n",
    "    x.append(img_to_array(img))  \n",
    "    x[c] = x[c].reshape((1,) + x[c].shape)  # this is a Numpy array with shape (1, 3, 150, 150\n",
    "    c=c+1\n",
    "\n",
    "\n",
    "image_x=images\n",
    "#create labels for training \n",
    "label=np.loadtxt('files/YT_train.csv', delimiter=',', dtype=np.int)\n",
    "label_train=label\n",
    "images=images[::-1]\n",
    "\n",
    "\n",
    "# the .flow() command below generates batches of randomly transformed images\n",
    "for a in range(0,len(images)):\n",
    "    i = 0\n",
    "    folder=\"healthy\";\n",
    "    if(label[a]==1):folder=\"unhealthy\";\n",
    "    #print(files[a]+\" \"+folder+\" \"+str(a))\n",
    "    for batch  in datagen.flow(x[a], batch_size=1,\n",
    "                              save_to_dir='images/SinusData/training/'+folder, save_prefix=\"sinus\", save_format='png'):\n",
    "        i += 1\n",
    "        if i > 20:\n",
    "            break  # otherwise the generator would loop indefinitely\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Dataset for Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator,img_to_array,load_img\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "       rotation_range=40,\n",
    "       width_shift_range=0.05,\n",
    "       height_shift_range=0.05,\n",
    "       rescale=1./255,\n",
    "       shear_range=0.2,\n",
    "       zoom_range=0.2,\n",
    "       horizontal_flip=True,\n",
    "       fill_mode='nearest')\n",
    "img_rows, img_cols = 200, 200\n",
    "\n",
    "\n",
    "listing = os.listdir(input_path_v)\n",
    "num_samples=len(listing)\n",
    "   \n",
    "x = []\n",
    "images=[]\n",
    "c=0\n",
    "for file in listing:\n",
    "    img = cv2.imread(input_path_v+\"/\"+file,0)  # this is a PIL image\n",
    "    images.append(img)\n",
    "    x.append(img_to_array(img))  # this is a Numpy array with shape (3, 150, 150)\n",
    "    x[c] = x[c].reshape((1,) + x[c].shape)  # this is a Numpy array with shape (1, 3, 150, 150\n",
    "    c=c+1\n",
    "\n",
    "\n",
    "# and saves the results to the `preview/` directory\n",
    "label=np.loadtxt('files/YV_train.csv', delimiter=',', dtype=np.int)\n",
    "\n",
    "# the .flow() command below generates batches of randomly transformed images\n",
    "for a in range(0,len(images)):\n",
    "    i = 0\n",
    "    folder=\"healthy\";\n",
    "    if(label[a]==1):folder=\"unhealthy\";\n",
    "    for batch  in datagen.flow(x[a], batch_size=1,\n",
    "                             save_to_dir='images/SinusData/validation/'+folder, save_prefix=\"sinus\", save_format='png'):\n",
    "        i += 1\n",
    "        if i > 20:\n",
    "            break  # otherwise the generator would loop indefinitely\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import statistics\n",
    "from statistics import variance\n",
    "\n",
    "def getFeatures(image):\n",
    "    height = image.shape[0]\n",
    "    width = image.shape[1]\n",
    "    w = 0\n",
    "    b = 0\n",
    "    g = 0\n",
    "    for i in range(0,height):\n",
    "        for j in range(0,width-1):\n",
    "            if (image[i][j]>180):\n",
    "                w = w+1\n",
    "\n",
    "            elif(image[i][j]<25):\n",
    "                b = b+1\n",
    "\n",
    "    area=image.shape[0]*image.shape[1]\n",
    "    g=area-(w+b)\n",
    "\n",
    "    (means, stds) = cv2.meanStdDev(image)\n",
    "    #stats = np.concatenate([means, stds]).flatten()\n",
    "\n",
    "    ent = entropy(image).item()\n",
    "\n",
    "    mean = np.mean(image).item()\n",
    "    median = np.median(image).item()\n",
    "    variance =np.var(image).item()\n",
    "    SD = np.std(image, axis=None, dtype=None, out=None, ddof=0, keepdims=False).item()\n",
    "    Skenss = ((mean - median)/SD)\n",
    "    \n",
    "    g=np.full((200,1),g)\n",
    "    b=np.full((200,1),b)\n",
    "    mean=np.full((200,1),mean)\n",
    "    median=np.full((200,1),median)\n",
    "    SD=np.full((200,1),SD)\n",
    "    variance=np.full((200,1),variance)\n",
    "    Skenss=np.full((200,1),Skenss)\n",
    "    ent=np.full((200,1),ent)\n",
    "    return g,b,mean,median,variance,Skenss,ent\n",
    "\n",
    "def entropy(signal):\n",
    "    signal = signal.ravel()\n",
    "    lensig = signal.size\n",
    "    symset = list(set(signal))\n",
    "    propab = [np.size(signal[signal == i]) / (1.0 * lensig) for i in symset]\n",
    "    ent = np.sum([p * np.log2(1.0 / p) for p in propab])\n",
    "    return ent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing X_train,Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from skimage.filters import sobel\n",
    "input_path_p=\"images/SinusData/training/healthy\"\n",
    "input_path_n=\"images/SinusData/training/unhealthy\"\n",
    "\n",
    "listing = os.listdir(input_path_p)\n",
    "dataset_size=200\n",
    "num_samples=len(listing)\n",
    "\n",
    "\n",
    "X_train=[]\n",
    "c=0\n",
    "for file in listing:\n",
    "    img = cv2.imread(input_path_p+\"/\"+file,0)  # this is a PIL image\n",
    "    #img=cv2.medianBlur(img,7)\n",
    "    features=getFeatures(img)\n",
    "    features=np.asarray(features)\n",
    "    #features.reshape(dataset_size,-2)\n",
    "    #for i in range(len(features)):img=np.hstack((img,features[i]))\n",
    "    X_train.append(features)\n",
    "    \n",
    "listing = os.listdir(input_path_n)\n",
    "len_pos=len(X_train)\n",
    "\n",
    "for file in listing:\n",
    "    img = cv2.imread(input_path_n+\"/\"+file,0)  # this is a PIL image\n",
    "    #img=cv2.medianBlur(img,7)\n",
    "    features=getFeatures(img)\n",
    "    features=np.asarray(features)\n",
    "    #features.reshape(dataset_size,-2)\n",
    "    #for i in range(len(features)):img=np.hstack((img,features[i]))\n",
    "    X_train.append(features)\n",
    "    \n",
    "pos_labels = np.zeros((len_pos, 1), dtype=int);  \n",
    "neg_labels = np.ones((len(X_train)-len_pos, 1), dtype=int);\n",
    "\n",
    "Y_train=np.concatenate((pos_labels, neg_labels))\n",
    "X_train=np.asarray(X_train)\n",
    "\n",
    "X_train = X_train.astype('float32')\n",
    "\n",
    "X_train = X_train.reshape((X_train.shape[0],) + X_train[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing X_test,Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "import numpy\n",
    "input_path_p=\"images/SinusData/validation/healthy\"\n",
    "input_path_n=\"images/SinusData/validation/unhealthy\"\n",
    "\n",
    "\n",
    "listing = os.listdir(input_path_p)\n",
    "num_samples=len(listing)\n",
    "\n",
    "\n",
    "\n",
    "X_test=[]\n",
    "c=0\n",
    "for file in listing:\n",
    "    img = cv2.imread(input_path_p+\"/\"+file,0)  # this is a PIL image\n",
    "    #img=cv2.medianBlur(img,7)\n",
    "    features=getFeatures(img)\n",
    "    features=numpy.asarray(features)\n",
    "    #features.reshape(dataset_size,-2)\n",
    "    #for i in range(len(features)):img=np.hstack((img,features[i]))\n",
    "    X_test.append(features)\n",
    "\n",
    "    \n",
    "listing = os.listdir(input_path_n)\n",
    "len_pos=len(X_test)\n",
    "\n",
    "\n",
    "for file in listing:\n",
    "    img = cv2.imread(input_path_n+\"/\"+file,0)  # this is a PIL image\n",
    "    #img=cv2.medianBlur(img,7)\n",
    "    features=getFeatures(img)\n",
    "    features=numpy.asarray(features)\n",
    "    #features.reshape(dataset_size,-2)\n",
    "    #for i in range(len(features)):img=np.hstack((img,features[i]))\n",
    "    X_test.append(features)\n",
    "    \n",
    "    \n",
    "pos_labels = np.zeros((len_pos, 1), dtype=int)  \n",
    "neg_labels = np.ones((len(X_test)-len_pos, 1), dtype=int)\n",
    "Y_test=np.concatenate((pos_labels, neg_labels))\n",
    "\n",
    "X_test=np.asarray(X_test)\n",
    "X_test = X_test.astype('float32')\n",
    "X_test = X_test.reshape((X_test.shape[0],) + X_test[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing X_train,X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train /= 255\n",
    "X_test /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print((X_test[1].shape))\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradinet Bossting Alogrithm Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_hastie_10_2\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import pickle\n",
    "d=0\n",
    "\n",
    "dataset_size = len(X_train)\n",
    "Xtrain = X_train.reshape(dataset_size,-1)\n",
    "#Xtrain=X_train\n",
    "#Xtrain=Xtrain/255\n",
    "\n",
    "print(Xtrain.shape)\n",
    "model = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1,max_depth=1, random_state=1).fit(Xtrain, np.ravel(Y_train,order='C'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save model to file\n",
    "pickle.dump(model, open(\"features.pickle1.dat\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean Square Error And Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error,r2_score,accuracy_score\n",
    "dataset_size = len(X_test)\n",
    "xtest = X_test.reshape(dataset_size,-1)\n",
    "mse = mean_squared_error(Y_test, model.predict(xtest))\n",
    "acc = r2_score(Y_test, model.predict(xtest))\n",
    "\n",
    "print(\"MSE: %.4f\" % mse)\n",
    "print('ACC: %.4f' % acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# predict class labels\n",
    "pred = model.predict(xtest)\n",
    "\n",
    "# score on test data (accuracy)\n",
    "acc = model.score(xtest, Y_test)\n",
    "print('ACC: %.4f' % acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predection Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "img_rows=200 \n",
    "img_cols=200\n",
    "img=cv2.imread(\"images/SingleCode/Labe2.png\",0);\n",
    "img = cv2.resize(img, (img_rows, img_cols), interpolation = cv2.INTER_CUBIC)\n",
    "print(img.shape)\n",
    "print(img.shape)\n",
    "features=getFeatures(img)\n",
    "features=numpy.asarray(features)\n",
    "h,u=model.predict_proba(features)[0]\n",
    "if(h>u):print(\"Healthy\",h,u);\n",
    "else:print(\"UnHelathy\",h,u);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Weight from File and Predict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before:  200\n",
      "features (7, 200, 1)\n",
      "features (1, 1400)\n",
      "After:  120000\n",
      "UnHelathy 0.24445836826396683 0.7555416317360332\n"
     ]
    }
   ],
   "source": [
    "# load model from file\n",
    "import pickle\n",
    "import cv2\n",
    "import numpy\n",
    "\n",
    "img_rows, img_cols= 200,200\n",
    "\n",
    "loaded_model = pickle.load(open(\"features.pickle1.dat\", \"rb\"))\n",
    "    \n",
    "img=cv2.imread(\"images/Portal/sinus_left5.png\");\n",
    "img = cv2.resize(img, (img_rows, img_cols), interpolation = cv2.INTER_CUBIC)\n",
    "print(\"Before: \",len(img[0]))\n",
    "img=img.reshape(1,-1)\n",
    "features=getFeatures(img)\n",
    "features=numpy.asarray(features)\n",
    "print(\"features\",features.shape)\n",
    "print(\"After: \",len(img[0]))\n",
    "h,u=loaded_model.predict_proba(features)[0]\n",
    "if(h>u):print(\"Healthy\",h,u);\n",
    "else:print(\"UnHelathy\",h,u);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "params = {'n_estimators': 100, 'max_depth': 1,\n",
    "        'learning_rate': 1, 'loss': 'huber','alpha':0.95}\n",
    "test_score = np.zeros((params['n_estimators'],), dtype=np.float64)\n",
    "\n",
    "for i, y_pred in enumerate(model.staged_decision_function(xtest)):\n",
    "    test_score[i] = model.loss_(Y_test, y_pred)\n",
    "    #print(test_score)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 1, 1)\n",
    "plt.title('Deviance')\n",
    "plt.plot(np.arange(params['n_estimators']) + 1, model.train_score_, 'b-',\n",
    "                label='Training Set Deviance')\n",
    "plt.plot(np.arange(params['n_estimators']) + 1, test_score, 'r-',\n",
    "                label='Test Set Deviance')\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlabel('Boosting Iterations')\n",
    "plt.ylabel('Deviance')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
